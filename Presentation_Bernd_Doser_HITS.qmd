---
title: Spherinator & HiPSter
subtitle: SPACE Training Webinar
author: Bernd Doser (HITS)
date: 2024/05/07
date-format: "MMMM YYYY"
# institute: "[HITS gGmbH](https://h-its.org)"
title-slide-attributes:
  data-background-image: images/SPACE_title_slide.png
format:
  revealjs:
    logo: images/HITS_RGB_eng.jpg
    footer: "Spherinator & HiPSter (B. Doser)"
    slide-number: true
    highlight-style: a11y
    bibliography: references.bib
    width: 1300
    template-partials:
      - title-slide.html
  pptx:
    reference-doc: Space_template_v04_20240216.pptx
---

## Outline {background-image="images/aladin-lite-background.png"}

- Development Environment
- Spherinator: The Model
- Spherinator: The Model Training
- HiPSter: The Inference Workflow
- Visualization with Aladin-Lite
- Summary and Outlook


## FAIR principle

- **F**indability
- **A**ccessibility
- **I**nteroperability
- **R**eusability


## Development Environment

- Python dependency management with [Poetry]()
- Automated dependency updates with [Dependabot]()
- Project management at [GitHub](https://github.com/HITS-AIN/Spherinator)
  - Pull requests for each update
- Continuous integration with [GitHub Action](https://github.com/HITS-AIN/Spherinator/blob/main/.github/workflows/python-package.yml)
  - Static code analysis
  - Unit tests
  - Pull requests: Check before merge


## Spherinator: The Model

- Representation learning by **dimensionality reduction**
- **Convolutional Variational Autoencoder** (VAE)
![](images/vae.png){.center}
- **Rotational invariance** by using set of rotated images, where only the one with the lowest reconstruction loss is used for the training
- **(Hyper-)spherical** latent space


## Power Spherical Distribution

$$\begin{aligned}
p_{X}(x; \mu, \kappa) = N_{X}(\kappa, d)^{-1}(1 + \mu^{\top}x)^{\kappa}
\end{aligned}$$


:::: {.columns}

::: {.column width="40%"}
$d$: Dimension

$\mu$: Direction

$\kappa$: Concentration

$N_{X}$: Normalization factor
:::

::: {.column width="60%"}
![](images/power_spherical.png)
:::

::::

::: aside
Source: [@DeCao2020]
:::


## Loss function

$$\begin{aligned}
L = L_{recon} + \lambda \cdot L_{KL}
\end{aligned}$$

$L_{recon}$: Reconstruction loss by pixel-wise euclidean distance 

$L_{KL}$: Kullback-Leibler divergence (measure of distribution differs from a reference distribution)

$\lambda$: Balancing factor

Euclidean space: Gaussian reference distribution

Hyper-spherical space: Uniform reference distribution


## PyTorch Lightning

Experiments defined in yaml file:

```yaml
{{< include _illustris_power_simplified.yaml >}}
```


## Weights&Biases: AI Developer Platform

- Tracking experiments
![](images/wandb_illustris_loss.png)
- Collaborative project management (teams, reports) 
- Model registry (publish models for staging and production)
- Hyperparameter optimization
- Free for academic research


## Visualization with Aladin(-Lite) {.nostretch}

HiPS - Hierarchical Progressive Survey

![](images/HEALPix.png){width="900"}

The HiPS tilings are visualized with Aladin Lite.
The HiPS tile center is 

::: aside
Source: [@Fernique_2015]
:::


## HiPSter Workflow {.nostretch}

- Python workflow for inference
![](images/HiPSter.drawio.svg){.absolute left=0 width=1800}


## Summary and Outlook

- Prototyp public available [space.h-its.org](https://space.h-its.org)
- Evaluation of encoder, decoder, loss function for better reconstruction 
- Orchestration of the ML and HiPSter workflow
  - [Flyte]()
  - [StreamFlow]()
- **Geometric Deep Learning**: 3D point clouds instead of images
- [ONNX](https://onnx.ai/): Open Standard for machine learning interoperability


## Acknowledgement & Disclaimer {background-image="images/SPACE_acknowledgement_background.png"}


## References
